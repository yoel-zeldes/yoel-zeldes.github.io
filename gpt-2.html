<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>The Accessibility of GPT-2 - Text Generation and Fine-tuning</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="https://anotherdatum.com/gpt-2.html" rel="canonical" />
  <!-- Feed -->
        <link href="https://anotherdatum.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Another Datum Full Atom Feed" />

  <link href="https://anotherdatum.com/theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="https://anotherdatum.com/theme/css/code_blocks/tomorrow.css" rel="stylesheet">

    <!-- CSS specified by the user -->


    <link href="https://anotherdatum.com/css/overrides.css" type="text/css" rel="stylesheet" />

  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->



    <meta name="description" content="Text generation using GPT-2 is quite easy, using the right tools. Learn how to do it, as well as how to fine-tune the model on your own dataset.">

    <meta name="author" content="Yoel Zeldes">

    <meta name="tags" content="deep learning">
    <meta name="tags" content="NLP">
    <meta name="tags" content="NLG">
    <meta name="tags" content="GPT-2">




<!-- Open Graph -->
<meta property="og:site_name" content="Another Datum"/>
<meta property="og:title" content="The Accessibility of GPT-2 - Text Generation and Fine-tuning"/>
<meta property="og:description" content="Text generation using GPT-2 is quite easy, using the right tools. Learn how to do it, as well as how to fine-tune the model on your own dataset."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://anotherdatum.com/gpt-2.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-11-28 23:00:00+02:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://anotherdatum.com/author/yoel-zeldes.html">
  <meta property="article:publisher" content="https://www.facebook.com/yoel.zeldes" />
<meta property="article:section" content="gpt-2"/>
<meta property="article:tag" content="deep learning"/>
<meta property="article:tag" content="NLP"/>
<meta property="article:tag" content="NLG"/>
<meta property="article:tag" content="GPT-2"/>
<meta property="og:image" content="https://anotherdatum.com/images/gpt-2/cover.jpg">

<!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@YZeldes">
    <meta name="twitter:title" content="The Accessibility of GPT-2 - Text Generation and Fine-tuning">
    <meta name="twitter:url" content="https://anotherdatum.com/gpt-2.html">

        <meta name="twitter:image:src" content="https://anotherdatum.com/images/gpt-2/cover.jpg">

      <meta name="twitter:description" content="Text generation using GPT-2 is quite easy, using the right tools. Learn how to do it, as well as how to fine-tune the model on your own dataset.">

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "The Accessibility of GPT-2 - Text Generation and Fine-tuning",
  "headline": "The Accessibility of GPT-2 - Text Generation and Fine-tuning",
  "datePublished": "2019-11-28 23:00:00+02:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Yoel Zeldes",
    "url": "https://anotherdatum.com/author/yoel-zeldes.html"
  },
  "image": "https://anotherdatum.com/images/gpt-2/cover.jpg",
  "url": "https://anotherdatum.com/gpt-2.html",
  "description": "Text generation using GPT-2 is quite easy, using the right tools. Learn how to do it, as well as how to fine-tune the model on your own dataset."
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>
          <li><a href="https://anotherdatum.com" role="presentation">Posts</a></li>

              <li role="presentation"><a href="https://anotherdatum.com/pages/about.html">about me</a></li>
              <li role="presentation"><a href="https://anotherdatum.com/pages/resources.html">Resources</a></li>

    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="https://anotherdatum.com/" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">The Accessibility of GPT-2 - Text Generation and Fine-tuning</h1>
        <!-- TODO : Proper class for headline -->
        <span class="post-meta">
            <time datetime="28 November 2019">28 November 2019</time>
        </span>
        <!-- TODO : Modified check -->
            <div class="post-cover cover" style="background-image: url('https://anotherdatum.com/images/gpt-2/cover.jpg')">
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <p>Natural Language Generation (NLG) is a well studied subject among the NLP community. With the rise of deep learning methods, NLG has become better and better. Recently, OpenAI has pushed the limits, with the release of <a href="https://openai.com/blog/better-language-models">GPT-2</a> - a <a href="https://arxiv.org/abs/1706.03762">Transformers</a> based model that predicts the next <a href="https://arxiv.org/abs/1508.07909">token</a> at each time space.</p>
<p>Nowadays itâ€™s quite easy to use these models - you donâ€™t need to implement the code yourself, or train the models using expensive resources. HuggingFace, for instance, has <a href="https://huggingface.co/transformers">released an API</a> that eases the access to the pretrained GPT-2 OpenAI has published. Some of its features include generating text, as well as fine-tuning the model on your own dataset - shifting the learned distribution so that the model will generate text from a new domain.</p>
<p>Doing all of these is easy - itâ€™s only a matter of pip installing the relevant packages and launching a python script. However, to save you the trouble, you could use one of the available platforms such as <a href="https://spell.run">Spell</a> - you just specify what you want to run, and Spell will take care of the rest (download the code, install the packages, allocate compute resources, manage results).</p>
<p>While not being a Spell advocate (I havenâ€™t even tried other features of the platform, or tried other platforms at all), I decided to write a tutorial that details the process Iâ€™ve just described. To find out more, you can find the tutorial <a href="https://community.spell.run/hc/en-us/articles/360038909713-GPT-2-text-generation-is-not-something-to-joke-about">here</a>.</p>
<p>If you also like to play around with machine generated text, feel free to leave a comment with interesting texts you got. :)</p>
<hr>
<p><em>UPDATE</em>: it seemsÂ the tutorial is no longer available in the aforementioned link. Although itâ€™s a bit outdated (the hugging face API has changed a lot since then), here is the full text:</p>
<p><br /></p>
<p>Natural Language Generation (NLG) is a well studied subject among the NLP community. One approach to tackle the challenge of text generation is to factorize the probability of a sequence of tokens (e.g. words or <a href="https://arxiv.org/abs/1508.07909">Byte Pair Encoding</a>) <span class="math">\(P(x_1, \ldots, x_n)\)</span> into the multiplication of the probabilities of getting each of the tokens <span class="math">\(x_1\)</span>, â€¦, <span class="math">\(x_n\)</span> conditioned on the tokens preceding it: <span class="math">\(\prod_{t=1}^{n}P(x_t|x_{&lt;t})\)</span>. Given a training dataset, one could train such a model to maximize the probability of the next token at each time step. Once the model has been trained, you could generate text by sampling from the distribution one token at a time. Easy as a breeze.</p>
<p>With the rise of deep learning methods, NLG has become better and better. Recently, OpenAI have pushed the limits, with the release of <a href="https://openai.com/blog/better-language-models">GPT-2</a>. This model uses the well known <a href="https://arxiv.org/abs/1706.03762">Transformers architecture</a>: in order to calculate the distribution over the next token, the model simultaneously uses the previous tokens using a self-attention mechanism.</p>
<p>Recently, HuggingFace have <a href="https://huggingface.co/transformers">released an API</a> easing the access to GPT-2. One of its features is generating text using the pre-trained model:</p>
<div class="highlight"><pre><span></span><code>spell run --github-url https://github.com/huggingface/transformers.git <span class="se">\</span>
  --pip tqdm <span class="se">\</span>
  --pip boto3 <span class="se">\</span>
  --pip requests <span class="se">\</span>
  --pip regex <span class="se">\</span>
  --pip sacremoses <span class="se">\</span>
  <span class="s2">&quot;python examples/run_generation.py \</span>
<span class="s2">    --model_type=gpt2 \</span>
<span class="s2">    --length=70 \</span>
<span class="s2">    --prompt=&#39; &#39; \</span>
<span class="s2">    --model_name_or_path=gpt2&quot;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>ðŸ’« Casting spell #1â€¦
âœ¨ Stop viewing logs with ^C
âœ¨ Machine_Requestedâ€¦ done
âœ¨ Buildingâ€¦ done
âœ¨ Run is running
â€¦
â€¦
â€¦
$5.30-$10.00

FREE SHIPPING

Items without a shipping address will be delivered to your confirmation email when you purchase your product.

Use &quot;POOL&quot; when ordering; deliveries to POOL addresses are completely separate from shipping.&lt;|endoftext|&gt;Earth&#39;s spin to new low the closer Earth takes to the Sun&#39;s
âœ¨ Savingâ€¦ done
âœ¨ Pushingâ€¦ done
ðŸŽ‰ Total run time: 1m7.057677s
ðŸŽ‰ Run 1 complete
</code></pre></div>

<p>That was easy! OpenAI have used diverse data found on the web for training the model, so the generated text can be pretty much any natural looking text. But what if instead of diversity, weâ€™d like to generate a specific kind of text? Letâ€™s try to generate jokes! To do so, weâ€™ll have to train the model using a dataset of jokes. Unfortunately, getting such a dataset would be ridiculously hard! To train GPT-2, which has 124M weights to be learned (and this is merely the smaller version of the architecture), we need a huge amount of data! But how are we going to get that many jokes? The short answer is: we wonâ€™t.</p>
<p>Learning to generate jokes involves learning how to generate natural-looking text, as well as making sure this text is funny. The first part is where most of the learning happens. Using the pre-trained version of GPT-2 as a starting point, the model wonâ€™t have to learn how to generate natural-looking text from scratch. All itâ€™ll have to learn is to concentrate the distribution over text that is funny. A relatively small dataset will do for the task.</p>
<p>Donâ€™t get me wrong, the dataset weâ€™ll be using isnâ€™t big enough to meaningfully learn anything useful. Moreover, training a model to generalize the concept of humor is a hard problem. However, for the purpose of this post - learning how to use and fine-tune a model such as GPT-2 - this will do: weâ€™ll witness how the dataset shifts the modelâ€™s distribution towards text that looks, to some extent, like jokes.</p>
<p>Weâ€™ll use one-liner jokes from <a href="https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv">short-jokes-dataset</a> to fine-tune GPT-2. Being shorter than the average joke, itâ€™ll be easier for the model to learn their distribution. So first thingâ€™s first, letâ€™s get the data:</p>
<div class="highlight"><pre><span></span><code>spell run <span class="s2">&quot;wget -O data.csv https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv &amp;&amp; python -c \&quot;import csv; f_in = open(&#39;data.csv&#39;, &#39;r&#39;); f_out = open(&#39;data.txt&#39;, &#39;w&#39;); f_out.write(&#39;\n&#39;.join(row[&#39;Joke&#39;] for row in csv.DictReader(f_in)))\&quot;&quot;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>ðŸ’« Casting spell #2â€¦
âœ¨ Stop viewing logs with ^C
âœ¨ Buildingâ€¦ done
âœ¨ Machine_Requestedâ€¦ done
âœ¨ Run is running
--2019-11-09 21:36:14--  https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/onelinefun.csv
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 253462 (248K) [text/plain]
Saving to: â€˜data.csvâ€™

     0K .......... .......... .......... .......... .......... 20% 3.34M 0s
    50K .......... .......... .......... .......... .......... 40% 6.72M 0s
   100K .......... .......... .......... .......... .......... 60%  167M 0s
   150K .......... .......... .......... .......... .......... 80%  122M 0s
   200K .......... .......... .......... .......... .......   100% 6.55M=0.03s

2019-11-09 21:36:14 (8.14 MB/s) - â€˜data.csvâ€™ saved [253462/253462]

âœ¨ Savingâ€¦ done
âœ¨ Pushingâ€¦ done
ðŸŽ‰ Total run time: 13.07418s
ðŸŽ‰ Run 2 complete
</code></pre></div>

<p>HuggingFace have already provided us with a script to fine-tune GPT-2:</p>
<div class="highlight"><pre><span></span><code>spell run --github-url https://github.com/huggingface/transformers.git <span class="se">\</span>
  --pip tqdm <span class="se">\</span>
  --pip boto3 <span class="se">\</span>
  --pip requests <span class="se">\</span>
  --pip regex <span class="se">\</span>
  --pip sacremoses <span class="se">\</span>
  -m runs/2/data.txt <span class="se">\</span>
  <span class="s2">&quot;python examples/run_lm_finetuning.py \</span>
<span class="s2">    --output_dir=output \</span>
<span class="s2">    --model_type=gpt2 \</span>
<span class="s2">    --model_name_or_path=gpt2 \</span>
<span class="s2">    --per_gpu_train_batch_size=2 \</span>
<span class="s2">    --num_train_epochs=10 \</span>
<span class="s2">    --do_train \</span>
<span class="s2">    --train_data_file=data.txt&quot;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>ðŸ’« Casting spell #3â€¦
âœ¨ Stop viewing logs with ^C
âœ¨ Machine_Requestedâ€¦ done
âœ¨ Buildingâ€¦ done
âœ¨ Mountingâ€¦ done
âœ¨ Run is running
â€¦
â€¦
â€¦
ðŸŽ‰ Total run time: 44h36m34.553059s
ðŸŽ‰ Run 3 complete
</code></pre></div>

<p>Note that the downloaded data from the previous run is mounted using the <code>-m</code> flag.
Even though weâ€™ve used a small dataset (3K examples), running 10 epochs on a CPU took about 44 hours. It only shows how big the model is. This is why you should use a GPU if you want to use a bigger dataset or run many experiments (e.g. tune hyper parameters).</p>
<p>Letâ€™s try to generate a joke, after mounting the result of the previous run:</p>
<div class="highlight"><pre><span></span><code>spell run --github-url https://github.com/huggingface/transformers.git <span class="se">\</span>
  --pip tqdm <span class="se">\</span>
  --pip boto3 <span class="se">\</span>
  --pip requests <span class="se">\</span>
  --pip regex <span class="se">\</span>
  --pip sacremoses <span class="se">\</span>
  -m runs/3/output <span class="se">\</span>
  <span class="s2">&quot;python examples/run_generation.py \</span>
<span class="s2">    --model_type=gpt2 \</span>
<span class="s2">    --length=70 \</span>
<span class="s2">    --prompt=&#39; &#39; \</span>
<span class="s2">    --model_name_or_path=output&quot;</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>ðŸ’« Casting spell #4â€¦
âœ¨ Stop viewing logs with ^C
âœ¨ Machine_Requestedâ€¦ done
âœ¨ Buildingâ€¦ done
âœ¨ Run is running
â€¦
â€¦
â€¦
&quot;I&#39;ve got seven fingers! But I don&#39;t have those!&quot;
Your childhood might be difficult, but at least it doesn&#39;t taste like your grandfather&#39;s.
Funny things never happen, in life.
Friends, We&#39;ve met on the beach. What&#39;s wrong with you?
If I&#39;m speaking honestly, I could use some
âœ¨ Savingâ€¦ done
âœ¨ Pushingâ€¦ done
ðŸŽ‰ Total run time: 51.047054s
ðŸŽ‰ Run 4 complete
</code></pre></div>

<p>The model has learned to generate short sentences, which is typical for our dataset. This relatively easy to grasp data statistic was well learned! Regarding how funny the model is - wellâ€¦ Iâ€™ll leave you to judge!</p>
<p><img alt="" src="images/gpt-2/joke.jpg"></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'Loading awesomeness...'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=The Accessibility of GPT-2 - Text Generation and Fine-tuning&amp;url=https://anotherdatum.com/gpt-2.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://anotherdatum.com/gpt-2.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="https://anotherdatum.com/tag/deep-learning.html">deep learning</a><a href="https://anotherdatum.com/tag/nlp.html">NLP</a><a href="https://anotherdatum.com/tag/nlg.html">NLG</a><a href="https://anotherdatum.com/tag/gpt-2.html">GPT-2</a>                </aside>

                <div class="clear"></div>


                </section>

<!-- Begin MailChimp Signup Form -->
<link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif;  width:300px;}
	#mc_embed_signup form{padding: 0;}
	/* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="https://anotherdatum.us14.list-manage.com/subscribe/post?u=6894d7badcfb253606fa3fb54&amp;id=c6f34ad6b7" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <div id="mc_embed_signup_scroll">
	<h2>Get updated of new posts</h2>
<div class="mc-field-group">
	<label for="mce-EMAIL">Email Address </label>
	<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_6894d7badcfb253606fa3fb54_c6f34ad6b7" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
<script type='text/javascript' src='//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js'></script><script type='text/javascript'>(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script>
<!--End mc_embed_signup-->
<hr />
                <aside class="post-nav">
                    <a class="post-nav-next" href="https://anotherdatum.com/gpt-3.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-left"></i>
                                <h2 class="post-nav-title">GPT-3, a Giant Step for Deep Learning andÂ NLP</h2>
                            <p class="post-nav-excerpt">Can intelligence emerge simply by training a big enough language model using lots of...</p>
                        </section>
                    </a>
                    <a class="post-nav-prev" href="https://anotherdatum.com/vae-moe.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-right"></i>
                                <h2 class="post-nav-title">Mixture of Variational Autoencoders - a Fusion Between MoE and VAE</h2>
                            <p class="post-nav-excerpt">An unsupervised approach to digit classification and generation.</p>
                        </section>
                    </a>
                    <div class="clear"></div>
                </aside>

                <div class="comments">
                    <h2>Comments !</h2>
                    <div id="disqus_thread"></div>
                    <script type="text/javascript">
                        var disqus_shortname = 'anotherdatum';
                        var disqus_identifier = 'gpt-2.html';
                        var disqus_url = 'https://anotherdatum.com/gpt-2.html';
                        (function() {
                            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                            dsq.src = '//anotherdatum.disqus.com/embed.js';
                            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                        })();
                    </script>
                    <noscript>Please enable JavaScript to view the comments.</noscript>
                </div>
            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
            <div class="social">
                <a href="https://il.linkedin.com/in/yoelzeldes">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-circle fa-stack-2x"></i>
                        <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                <a href="https://github.com/yoel-zeldes">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-circle fa-stack-2x"></i>
                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                <a href="https://www.facebook.com/yoel.zeldes">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-circle fa-stack-2x"></i>
                        <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
                <a href="https://twitter.com/YZeldes">
                    <span class="fa-stack fa-lg">
                        <i class="fa fa-circle fa-stack-2x"></i>
                        <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                    </span>
                </a>
            </div>

      <div class="inner">
        <section class="credits">
          <span class="credits-theme">Have a look at <a href="https://github.com/yoel-zeldes/yoel-zeldes.github.io/tree/source">the source code</a> of this blog.</span>
        </section>
      </div>
    </footer>
  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script type="text/javascript" src="https://anotherdatum.com/theme/js/script.js"></script>

    <!-- Global Site Tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-83684090-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-83684090-1', { 'anonymize_ip': true });
    </script>
<script type="text/javascript">
    var disqus_shortname = 'anotherdatum';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>